{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "\n",
    "The website scraped is the Official Gazette of the Philippines (https://www.officialgazette.gov.ph/section/executive-orders/), the country's repository of legal documents. In particular, the section on executive orders. This site was chosen because legal documents are typically not hosted on APIs, and the site itself contains this information in a structured manner across different portals. Access to timely and effective jurisprudence is key to several undertakings that aim to democratize access to legal information, as well as a bevy of research into the natural-language processes of legal domains using AI (Dyevre, 2021; Ibarra & Revilla, 2014; Peramo et al., 2021; Virtucio et al., 2018). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The website was scraped at <insert time and date here>. The robots text can be seen here: <insert img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building and Running the Selenium Scraper \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importing libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the necessary libraries \n",
    "\n",
    "from selenium import webdriver \n",
    "from selenium.webdriver.support.ui import WebDriverWait \n",
    "from selenium.webdriver.chrome.options import Options \n",
    "from selenium.webdriver.common.by import By \n",
    "from time import sleep \n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager \n",
    "import pandas as pd \n",
    "from bs4 import BeautifulSoup\n",
    "import requests "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initializing Selenium "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting target-page\n",
    "base_url = \"https://www.officialgazette.gov.ph/section/executive-orders/\"\n",
    "\n",
    "# # window settings - UNCOMMENT after running the noteboko fully\n",
    "# options = webdriver.ChromeOptions()\n",
    "# options.binary_location = \"\"\n",
    "# options.add_argument(\"--headless\")\n",
    "# options.add_argument(\"--start-maximized\")\n",
    "# options.add_argument(\"--incognito\")\n",
    "\n",
    "# initializing driver options \n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "driver.get(base_url)\n",
    "sleep(3) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scraping the relevant content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# this is the main div that contains the body. This returns a list of objects containing the EOs. There are 10 EOs on each page/body.\n",
    "main_body = driver.find_element(by=By.XPATH, value=\"/html/body/div[2]/section/main/div/div[1]\")  \n",
    "\n",
    "# contains each EO body. Nested underneath this are: 1) headers containing metadata & the EO name, 2) a brief summary of the EO, and 3) a footer containing category and tag links\n",
    "eo_articles = main_body.find_elements(by=By.TAG_NAME, value='article') \n",
    "\n",
    "data_list = []\n",
    "\n",
    "# iterating through 10 pages to get 100 observations:\n",
    "for i in range(11): \n",
    "\n",
    "# iterate through each executive order collection and extract relevant features\n",
    "    for eo in eo_articles: \n",
    "        # title\n",
    "        entry_title = eo.find_element(by=By.CLASS_NAME, value='entry-title') # header value to extract subdomains\n",
    "        title = entry_title.text # E.G. Executive Order No. 5, s. 2022\n",
    "\n",
    "        # date signed (metadata)\n",
    "        signed_on = eo.find_element(by=By.TAG_NAME, value='time').text # E.G. September 22, 2022 \n",
    "        \n",
    "        # url \n",
    "        url = eo.find_element(by=By.TAG_NAME, value='a').get_attribute('href') # E.G. https://www.officialgazette.gov.ph/section/executive-orders/...\n",
    "\n",
    "        # summary \n",
    "        summary = eo.find_element(by=By.TAG_NAME, value='p').text # E.G. TRANSFERERING THE ATTACHMENT OF TECHNICAL EDUCAITON AND SKILLS DEPT FROM ...\n",
    "\n",
    "        # category and tags. \n",
    "        categories_posted = eo.find_elements(by=By.CLASS_NAME, value='cat-links') # contains information on what type of legal document/categories this piece is filed under\n",
    "        cats_list = [cats.text for cats in categories_posted] # E.G. ['Executive Issuances, Executive Orders, Laws and Issuances']\n",
    "\n",
    "        tags_posted = eo.find_elements(by=By.CLASS_NAME, value='tag-links') # contains info on the relevant tag links for the type of issuance\n",
    "        tags_list = [tags.text for tags in tags_posted] # E.G. ['Tagged Executive Issuances, Executive Orders, Ferdinand R. Marcos Jr.']\n",
    "\n",
    "        # each row of the data frame will contain the following info:\n",
    "        data_dict = {'title': title, \n",
    "                    'signed_on': signed_on, \n",
    "                    'url': url, \n",
    "                    'summary': summary, \n",
    "                    'categories': cats_list, \n",
    "                    'tags': tags_list}\n",
    "\n",
    "        # appending the row to the existing dataframe \n",
    "        data_list.append(data_dict)\n",
    "\n",
    "        # clicking on the older issuances tag to get a fresh set of pages \n",
    "        older_issuances_button = driver.find_element(by=By.XPATH, value='/html/body/div[2]/section/main/div/div[1]/div/div/nav/div/div')\n",
    "        older_issuances_button.click()\n",
    "\n",
    "        ## testing the loop logic -- comment/uncomment as needed\n",
    "        # print(title)\n",
    "        # print(url)\n",
    "        # print(signed_on)\n",
    "        # print(summary)\n",
    "        # print(cats_list)\n",
    "        # print(tags_list)\n",
    "        # print('----') \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 110 entries, 0 to 109\n",
      "Data columns (total 6 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   title       110 non-null    object\n",
      " 1   signed_on   110 non-null    object\n",
      " 2   url         110 non-null    object\n",
      " 3   summary     110 non-null    object\n",
      " 4   categories  110 non-null    object\n",
      " 5   tags        110 non-null    object\n",
      "dtypes: object(6)\n",
      "memory usage: 5.3+ KB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.DataFrame(data_list)\n",
    "df.shape\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Clicking on the next page \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are 10 documents per page, so we can set a for loop with range 1-10 to collect our 100 data points \n",
    "\n",
    "# find the lick to older entries and have the page click on it as it scrapes through the necessary content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References \n",
    "\n",
    "Dyevre, A. (2021). Text-mining for lawyers: How machine learning techniques can advance our understanding of legal discourse. Erasmus Law Review, 14, 7. https://heinonline.org/HOL/Page?handle=hein.journals/erasmus14&id=9&div=&collection=\n",
    "\n",
    "Ibarra, V. C., & Revilla, C. D. (2014). Consumers’ awareness on their eight basic rights: A comparative study of filipinos in the philippines and guam (SSRN Scholarly Paper No. 2655817). Social Science Research Network. https://papers.ssrn.com/abstract=2655817\n",
    "\n",
    "Peramo, E., Cheng, C., & Cordel, M. (2021). Juris2vec: Building word embeddings from philippine jurisprudence. 2021 International Conference on Artificial Intelligence in Information and Communication (ICAIIC), 121–125. https://doi.org/10.1109/ICAIIC51459.2021.9415251\n",
    "\n",
    "Virtucio, M. B. L., Aborot, J. A., Abonita, J. K. C., Aviñante, R. S., Copino, R. J. B., Neverida, M. P., Osiana, V. O., Peramo, E. C., Syjuco, J. G., & Tan, G. B. A. (2018). Predicting decisions of the philippine supreme court using natural language processing and machine learning. 2018 IEEE 42nd Annual Computer Software and Applications Conference (COMPSAC), 02, 130–135. https://doi.org/10.1109/COMPSAC.2018.10348"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
